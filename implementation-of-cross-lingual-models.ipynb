{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-10T01:58:58.779157Z","iopub.execute_input":"2023-01-10T01:58:58.780692Z","iopub.status.idle":"2023-01-10T01:58:58.810366Z","shell.execute_reply.started":"2023-01-10T01:58:58.780544Z","shell.execute_reply":"2023-01-10T01:58:58.809430Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### PAPER\nUsing the understanding of the papers both proposed by Facebook AI: \n1.     Unsupervised Cross-lingual Representation Learning at Scale (2020) \n         XLM-R, a multilingual masked language model trained on 100 languages\n2.     Cross-lingual Language Model Pretraining(2019)\n        XLM uses a pre-processing technique (Byte Pairing Encoding) to create a shared a shared vocabulary between two languages.\n        The bias is removed from the low-resource langauages by sampling using a multinomial distribution. And a dual-language training mechanism with BERT in order to learn relations between words in different languages.\n    \nThe goal was to \"expose the surprising effectiveness of multilingual models over monolingual models, and show\nstrong improvements on low-resource languages\"","metadata":{}},{"cell_type":"markdown","source":"### STRUCTURE OF THE TRANSFORMER\nBased off the \"Attention is all you need\" paper. Transfromers are able to handle long range dependencies by focusing on parts of our input sequence while we predicted our output sequence.\n\n1. ENCODER: maps an input sequence of symbol representations (x₁, …, xₙ) to a sequence of representations z = (z₁, …, zₙ). Each encoder has two sub-layers.\n    * A multi-head self attention mechanism on the input vectors. Helping the encoder look at others as it encodes a specific word. \n    * A simple, position-wise fully connected feed-forward network that does the post-processing. \n    \n    \n2. DECODER: Given z, the decoder then generates an output sequence (y₁, …, yₘ) of symbols one element at a time.\n    * A masked multi-head self attention mechanism on the output vectors of the previous iteration.\n    * A multi-head attention mechanism on the output from encoder and masked multi-headed attention in decoder. (encoder-decoder attention layer that helps the decoder focus on relevant parts of the input sentence)\n    * A simple, position-wise fully connected feed-forward network   ","metadata":{}},{"cell_type":"markdown","source":"### Methods introduced based off the BERT architecture\n\n1. Each training sample consists of a shared vocabulary of the same text in two languages.  In BERT, each sample is built on a mono-lingual dataset. \n2. As in BERT, the goal of the model is to predict the masked tokens. The XLM model predicts the masked word in one language by either attending to the neighbouring words in either of the two lanaguages, encouraging the model to align the representations in both languages. Also, the XLM model can leverage context from one language to predict the masked word in another language, if the expected language is not sufficent for inference. \n\nThe upgraded BERT is denoted as **Translation Language Modeling (TLM)** while the “vanilla” BERT with BPE inputs is denoted as Masked Language Modeling (MLM).","metadata":{}},{"cell_type":"markdown","source":"## TPU SETUP","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport gc\nimport os\nimport time\nimport math\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom datetime import date\nfrom transformers import *\nfrom sklearn.metrics import *\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport gc\nimport re\nimport folium\n#import textstat\nfrom scipy import stats\nfrom colorama import Fore, Back, Style, init\n\nimport math\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\nimport random\nimport networkx as nx\nfrom pandas import Timestamp\n\nfrom PIL import Image\nfrom IPython.display import SVG\n#from keras.utils import model_to_dot\n\nimport requests\nfrom IPython.display import HTML\n\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ntqdm.pandas()\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport transformers\nimport tensorflow as tf\n\nfrom tensorflow.keras.callbacks import Callback\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n\nfrom tensorflow.keras.models import Model\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras.optimizers import Adam\nfrom tokenizers import BertWordPieceTokenizer\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Embedding\nfrom tensorflow.keras.layers import LSTM, GRU, Conv1D, SpatialDropout1D\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import activations\nfrom tensorflow.keras import constraints\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\n\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.activations import *\nfrom tensorflow.keras.constraints import *\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras.regularizers import *\n\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle\nfrom gensim.models import Word2Vec\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer,\\\n                                            CountVectorizer,\\\n                                            HashingVectorizer\n\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import TweetTokenizer  \n\nimport nltk\nfrom textblob import TextBlob\n\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\n#from googletrans import Translator\nfrom nltk import WordNetLemmatizer\n#from polyglot.detect import Detector\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nstopword=set(STOPWORDS)\n\nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\n\nnp.random.seed(0)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-01-10T01:58:58.811954Z","iopub.execute_input":"2023-01-10T01:58:58.812533Z","iopub.status.idle":"2023-01-10T01:59:18.961769Z","shell.execute_reply.started":"2023-01-10T01:58:58.812489Z","shell.execute_reply":"2023-01-10T01:59:18.960252Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync","metadata":{"execution":{"iopub.status.busy":"2023-01-10T01:59:18.964288Z","iopub.execute_input":"2023-01-10T01:59:18.965072Z","iopub.status.idle":"2023-01-10T01:59:18.981041Z","shell.execute_reply.started":"2023-01-10T01:59:18.965024Z","shell.execute_reply":"2023-01-10T01:59:18.979610Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Tensorflow version 2.6.4\nREPLICAS:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}